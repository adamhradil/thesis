{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=missing-module-docstring, missing-function-docstring, missing-class-docstring, unused-argument\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "from scrapy import signals  # type: ignore\n",
    "from scrapy.crawler import CrawlerProcess  # type: ignore\n",
    "from scrapy.exporters import JsonItemExporter  # type: ignore\n",
    "\n",
    "import pandas as pd  # type: ignore\n",
    "\n",
    "from geopy.geocoders import Nominatim  # type: ignore\n",
    "from geopy.distance import geodesic  # type: ignore\n",
    "from database_wrapper import DatabaseWrapper\n",
    "\n",
    "from bezrealitky_scraper.bezrealitky.spiders.search_flats import SearchFlatsSpider\n",
    "from sreality_scraper.sreality.spiders.sreality_spider import SrealitySpider\n",
    "\n",
    "from listing import Disposition, UserPreferences, Listing\n",
    "\n",
    "# from sreality_scraper.sreality.spiders.sreality_spider import SrealitySpider\n",
    "\n",
    "items = []\n",
    "\n",
    "\n",
    "def get_coordinates(address):\n",
    "    geolocator = Nominatim(user_agent=\"distance_calculator\")\n",
    "    location = geolocator.geocode(address)\n",
    "    if location:\n",
    "        return (location.latitude, location.longitude)  # type: ignore\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_distance(address1, address2):\n",
    "    coord1 = get_coordinates(address1)\n",
    "    coord2 = get_coordinates(address2)\n",
    "\n",
    "    if coord1 and coord2:\n",
    "        return geodesic(coord1, coord2).kilometers\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_listings(listings):\n",
    "    cleaned_listings = []\n",
    "    seen_listings = set()\n",
    "\n",
    "    for listing in listings:\n",
    "        # Remove duplicates\n",
    "        if str(listing) in seen_listings:\n",
    "            continue\n",
    "        seen_listings.add(str(listing))\n",
    "\n",
    "        # Handle missing values\n",
    "        for attr, value in listing.__dict__.items():\n",
    "            if value == \"\":\n",
    "                listing.__dict__[attr] = None  # or some default value\n",
    "\n",
    "        # Validate data types\n",
    "        # This is just an example for the 'area' attribute\n",
    "        if listing.area is not None:\n",
    "            try:\n",
    "                listing.area = int(listing.area)\n",
    "            except ValueError:\n",
    "                continue  # skip this listing\n",
    "\n",
    "        # Normalize text\n",
    "        if listing.description is not None:\n",
    "            listing.description = listing.description.lower().strip()\n",
    "\n",
    "        cleaned_listings.append(listing)\n",
    "\n",
    "    return cleaned_listings\n",
    "\n",
    "\n",
    "def balcony_filter(listings: list[Listing]):\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    l3 = []\n",
    "    l4 = []\n",
    "    for advert in listings:\n",
    "        if \"balk\" in advert.description.lower():\n",
    "            l1.append(advert)\n",
    "        if advert.balcony:\n",
    "            l2.append(advert)\n",
    "        if \"balk\" in advert.description.lower() and advert.balcony:\n",
    "            l3.append(advert)\n",
    "        if \"balk\" in advert.description.lower() and not advert.balcony:\n",
    "            l4.append(advert)\n",
    "\n",
    "    print(f\"{len(l1)} listings contain balk in description\")\n",
    "    print(f\"{len(l2)} listings contain contain balk in the table\")\n",
    "    print(f\"{len(l3)} listings contain contain balk in description and in table\")\n",
    "    print(f\"{len(l4)} listings contain contain balk in description and not in table\")\n",
    "    return\n",
    "\n",
    "\n",
    "def item_scraped(item):\n",
    "    print(item[\"url\"])\n",
    "    items.append(item)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    CRAWL = False\n",
    "    # FILE = \"bezrealitky_items.json\"\n",
    "    # FILE = \"sreality_items.json\"\n",
    "    FILE = \"all_items.json\"\n",
    "    POI = \"NTK Praha\"\n",
    "    start = 0.0\n",
    "    end = 0.0\n",
    "\n",
    "    crawl_time = datetime.datetime.now()\n",
    "\n",
    "    if CRAWL:\n",
    "        process = CrawlerProcess(\n",
    "            settings={\n",
    "                \"LOG_LEVEL\": \"INFO\",\n",
    "                \"DEFAULT_REQUEST_HEADERS\": {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        crawler = process.create_crawler(SearchFlatsSpider)\n",
    "        crawler.signals.connect(item_scraped, signal=signals.item_scraped)\n",
    "        crawler2 = process.create_crawler(SrealitySpider)\n",
    "        crawler2.signals.connect(item_scraped, signal=signals.item_scraped)\n",
    "        process.crawl(crawler)\n",
    "        process.crawl(crawler2)\n",
    "        process.start()\n",
    "\n",
    "        with open(file=FILE, mode=\"wb\") as f:\n",
    "            exporter = JsonItemExporter(f)\n",
    "            exporter.start_exporting()\n",
    "            for i in items:\n",
    "                exporter.export_item(i)\n",
    "            exporter.finish_exporting()\n",
    "\n",
    "        end = time.time()\n",
    "    else:\n",
    "        with open(FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            items = json.load(f)\n",
    "\n",
    "    listings = []\n",
    "    for i in items:\n",
    "        listings.append(Listing(i))\n",
    "\n",
    "    dist = calculate_distance(POI, items[0][\"address\"])\n",
    "\n",
    "    preferences = UserPreferences(\n",
    "        dispositions=[\n",
    "            Disposition.TWO_PLUS_ONE,\n",
    "            Disposition.THREE_PLUS_KK,\n",
    "            Disposition.THREE_PLUS_ONE,\n",
    "        ],\n",
    "        weight_area=0.5,\n",
    "        weight_rent=0.4,\n",
    "        weight_location=0.1,\n",
    "        min_area=50,\n",
    "        max_price=30000,\n",
    "        balcony=True,\n",
    "    )\n",
    "\n",
    "    balcony_filter(listings)\n",
    "\n",
    "    listings = clean_listings(listings=listings)\n",
    "\n",
    "    db = DatabaseWrapper(\"listings.db\")\n",
    "    db.create_table()\n",
    "    # if not db.verify_table_columns():\n",
    "    #     print(\"Table columns are not correct\")\n",
    "    #     sys.exit(1)\n",
    "    for listing in listings[:10]:\n",
    "        found_listing = db.get_listing(listing.id)\n",
    "        if found_listing:\n",
    "            if found_listing != listing:\n",
    "                print(f\"listing {listing.id} has changed\")\n",
    "                db.update_listing(listing, created=found_listing.created, date_updated=crawl_time, last_seen=crawl_time)\n",
    "            else:\n",
    "                db.update_listing(listing, created=found_listing.created, date_updated=found_listing.updated, last_seen=crawl_time)\n",
    "            continue\n",
    "        db.insert_listing(listing=listing, date_created=crawl_time)\n",
    "        print(f\"found a new listing: {listing.id}\")\n",
    "    df = db.get_df()\n",
    "    db.close_conn()\n",
    "\n",
    "    if start != 0.0 and end != 0.0:\n",
    "        print(f\"crawling finished in {end - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique disposition values\n",
    "print(df[\"disposition\"].unique())\n",
    "dispositions = df[\"disposition\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sreality_scraper.sreality.spiders.sreality_spider import SrealityUrlBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each unique disposition value try running the map_category_sub_cb from SrealityUrlBuilder\n",
    "for disposition in dispositions:\n",
    "    if not disposition:\n",
    "        continue\n",
    "    \n",
    "    df['disposition'] = df['disposition'].replace(disposition, SrealityUrlBuilder.map_category_sub_cb(disposition))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disposition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disposition'] = df['disposition'].replace('Garsoniéra', '1+kk') # bezrealitky specific\n",
    "df['disposition'] = df['disposition'].replace('Ostatní', 'ostatní') # bezrealitky specific\n",
    "df['disposition'] = df['disposition'].replace('atypicky', 'ostatní') # sreality specific\n",
    "df['disposition'] = df['disposition'].replace('pokoj', 'ostatní') # sreality specific\n",
    "df['disposition'] = df['disposition'].replace('6+kk', '6-a-více')\n",
    "df['disposition'] = df['disposition'].replace('6+1', '6-a-více')\n",
    "df['disposition'] = df['disposition'].replace('7+kk', '6-a-více')\n",
    "df['disposition'] = df['disposition'].replace('7+1', '6-a-více')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['disposition'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop entries with area bigger than 1000\n",
    "df = df[df['area'] < 1000]\n",
    "df['area'].sort_values()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
